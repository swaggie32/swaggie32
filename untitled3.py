# -*- coding: utf-8 -*-
"""Untitled3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/138JAxqnO1cm1qVTYXIB4j7S67L0Upoq8
"""

from google.colab import drive

drive.mount('/content/gdrive')

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
# %matplotlib inline
import seaborn as sns
from sklearn.cluster import KMeans

dataset  =  pd.read_excel(r"/content/gdrive/My Drive/content/2255872-anime_data.xlsx")

dataset.head(2)

dataset.shape

dataset.columns

dataset.eps.describe()

dataset[ (dataset['eps'] > 24) & (dataset.duration.isna())].shape

dataset_excluding_out = dataset[dataset['eps'] < 50]

dataset_excluding_out['eps_brackets']=pd.cut(dataset_excluding_out['eps'], bins=[1,10,20,30,40,50],\
                                             labels = ['cat1', 'cat2', 'cat3', 'cat4', 'cat5'])

dataset_excluding_out.shape

dataset_excluding_out.groupby(['eps_brackets']).duration.mean()

dataset_excluding_out[dataset_excluding_out['eps_brackets']=='cat1'].shape

dataset[(dataset['eps'] < 24) & (-dataset.duration.isna())].describe()

dataset_excluding_out.groupby('mediaType').agg({'duration':'mean', 'mediaType':'count'})

dataset.isna().sum()

dataset.describe()

def continuous_univariate_analysis(data, feature, figsize=(12, 2), kde=False, bins=None):
    # Create subplots with shared x-axis
    f1, (ax_box, ax_hist) = plt.subplots(
        nrows=2,
        sharex=True,
        gridspec_kw={"height_ratios": (0.25, 0.75)},
        figsize=figsize
    )

    # Set color palette
    sns.color_palette("viridis", as_cmap=True)

    # Create a box plot
    sns.boxplot(data=data, x=feature, ax=ax_box, showmeans=True, color="yellow")

    # Create a histogram
    if bins:
        sns.histplot(data=data, x=feature, ax=ax_hist, showmeans=True, color="crest", bins=bins, kde=kde)
    else:
        sns.histplot(data=data, x=feature, ax=ax_hist, kde=kde, color="blue")

    # Add vertical lines for mean and median
    ax_hist.axvline(data[feature].mean(), color='cyan', linestyle='--')
    ax_hist.axvline(data[feature].median(), color='orange', linestyle='--')

def discrete_univariate_analysis(data, feature, perc=False, n=None):
    total = len(data[feature])
    count = data[feature].nunique()

    if n is None:
        plt.figure(figsize=(count + 1, 5))
    else:
        plt.figure(figsize=(n + 1, 5))

    plt.xticks(rotation=90, fontsize=15)

    ax = sns.countplot(
        data=data,
        x=feature,
        palette="flare",
        order=data[feature].value_counts().index[:n].sort_values(ascending=False)
    )

    for p in ax.patches:
        if perc:
            label = "{:.1f}%".format(100 * p.get_height() / total)
        else:
            label = p.get_height()

        x = p.get_x() + p.get_width() / 2
        y = p.get_height()

        ax.annotate(
            label,
            (x, y),
            ha="center",
            va="center",
            size=12,
            xytext=(0, 5),
            textcoords="offset points"
        )

    plt.show()

dataset.drop(columns=['title','description'],axis=1,inplace=True)

dataset.head()

dataset.columns

dataset.rating.describe()

dataset.dropna(inplace=True)
dataset.shape

12000-7465

continuous_univariate_analysis(dataset,'rating')

continuous_univariate_analysis(dataset,'duration')

dataset[dataset['duration']>=80]['rating'].mean()

dataset[dataset['duration']>=100]['rating'].mean()

dataset[dataset['duration']>=110]['rating'].mean()

dataset[(dataset['duration']>=5) & (dataset['duration']>=30)]['rating'].mean()

discrete_univariate_analysis(dataset,"ongoing",perc=True)

dataset[dataset['ongoing']== True]['rating'].mean()

dataset[dataset['ongoing']== True]['duration'].mean()

discrete_univariate_analysis(dataset,"sznOfRelease",perc=True)

discrete_univariate_analysis(dataset,"studio_primary",perc=True)

discrete_univariate_analysis(dataset,"contentWarn",perc=True)

corr_cols =[item for item in dataset.columns if "tag" not in item]

corr_cols

corr_cols = [col for col in corr_cols if pd.api.types.is_numeric_dtype(dataset[col])]

plt.figure(figsize=(16,7))

sns.heatmap(dataset[corr_cols].corr(), annot=True, vmin=-1, vmax =1, fmt='.2f', cmap='Spectral')

plt.show()

plt.figure(figsize=(15,8))
sns.boxplot(x='sznOfRelease', y='rating',data=dataset)

x=dataset.drop(['rating'] ,axis=1)
y=dataset['rating']

x.info()

x = pd.get_dummies(x, columns=x.select_dtypes(include=['object', 'category']).columns.tolist(), drop_first=True)
x.head()

x.drop(columns='ongoing',inplace=True)

x.info()

from sklearn.model_selection import train_test_split

from sklearn.linear_model import LinearRegression

from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
from sklearn.ensemble import HistGradientBoostingClassifier

X_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size=0.2, random_state=1)

print("number of samples for train", X_train.shape[0])
print("number of samples for test", X_test.shape[0])

x.fillna(method='bfill')

x.isnull()
x.isnull().sum().sum()
x.dropna(inplace=True)

x.info()

lin_model = LinearRegression()
lin_model.fit(X_train, Y_train)

def Model_performance(model, predictor, target):
    pred = model.predict(predictor)
    r2 = r2_score(target, pred)

    rmse = np.sqrt(mean_squared_error(target, pred))

    results = pd.DataFrame({
        "RMSE": rmse,
        "R2 Score": r2
    },index=[0]
                   )

    return results

print("Training Data Performance")

lin_model_train = Model_performance(lin_model, X_train, Y_train)

print(lin_model_train)

print("Test Data Performance")

lin_model_test = Model_performance(lin_model, X_test, Y_test)

print(lin_model_test)

from mlxtend.feature_selection import SequentialFeatureSelector as SFS

reg = LinearRegression()
sfs = SFS(reg,
          k_features=X_train.shape[1],
          forward=True,
          floating=False,
          scoring='r2',
          n_jobs=-1,
          cv=5
         )

sfs = sfs.fit(X_train, Y_train)

from mlxtend.plotting import plot_sequential_feature_selection as plot_sfs

fig = plot_sfs(sfs.get_metric_dict(), kind='std_err', figsize=(15, 5))
plt.title('Feature Selector SFS')
plt.xticks(rotation=90)
plt.show()

from mlxtend.feature_selection import SequentialFeatureSelector as SFS

reg = LinearRegression()

sfs = SFS(
    reg,
    k_features=35,
    forward=True,
    floating=False,
    scoring='r2',
    n_jobs=-1,
    cv=5
)

sfs = sfs.fit(X_train, Y_train)

from mlxtend.plotting import plot_sequential_feature_selection as plot_sfs

fig = plot_sfs(sfs.get_metric_dict(), kind='std_err', figsize=(15, 5))
plt.title("Feature Selector - SFS")
plt.xticks(rotation=90)
plt.show()

feature_index = list(sfs.k_feature_idx_)
print(feature_index)

X_train.columns[feature_index]

X_train_final = X_train.iloc[:, feature_index]

X_train_final = X_train.iloc[:, feature_index]


X_test_final = X_test.iloc[:, feature_index]

lin_model_v2 = LinearRegression()
lin_model_v2.fit(X_train_final, Y_train)

print("Training Data Performance")

lin_model_train = Model_performance(lin_model, X_train, Y_train)

print(lin_model_train)

print("Training Data Performance")

lin_model_train = Model_performance(lin_model, X_test, Y_test)

print(lin_model_train)

X_train.columns[feature_index]

X_train.head(2)